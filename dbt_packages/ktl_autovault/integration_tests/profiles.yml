integration_tests:
  target: dev
  outputs:
    dev:
      type: spark
      method: session
      host: NA
      schema: duytk_test
      server_side_parameters:
        spark.master: local[*]
        spark.submit.deployMode: client
        spark.driver.extraClassPath: /home/dev/duy/jars/hadoop-aws-3.3.4.jar:/home/dev/duy/jars/iceberg-aws-bundle-1.5.2.jar:/home/dev/duy/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar
        spark.executor.extraClassPath: /home/dev/duy/jars/hadoop-aws-3.3.4.jar:/home/dev/duy/jars/iceberg-aws-bundle-1.5.2.jar:/home/dev/duy/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar
        spark.hadoop.hive.metastore.uris: thrift://192.168.1.156:9083
        spark.sql.warehouse.dir: s3a://data/duytk/warehouse
        spark.hadoop.fs.s3a.endpoint: http://192.168.1.151
        spark.hadoop.fs.s3a.path.style.access: "true"
        spark.hadoop.fs.s3a.ssl.enabled: "false"
        spark.hadoop.fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
        spark.hadoop.fs.s3a.aws.credentials.provider: com.amazonaws.auth.EnvironmentVariableCredentialsProvider
        spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version: "2"
        spark.sql.extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
        spark.sql.catalog.spark_catalog: org.apache.iceberg.spark.SparkSessionCatalog
        spark.sql.catalog.spark_catalog.type: hive
        spark.sql.catalog.demo: org.apache.iceberg.spark.SparkCatalog
        spark.sql.catalog.demo.type: hive
        spark.sql.catalog.demo.uri: thrift://192.168.1.156:9083
        spark.sql.catalog.demo.io-impl: org.apache.iceberg.aws.s3.S3FileIO
        spark.sql.catalog.demo.s3.endpoint: http://192.168.1.151
        spark.sql.catalog.demo.warehouse: s3a://data/duytk/warehouse
        spark.sql.defaultCatalog: spark_catalog